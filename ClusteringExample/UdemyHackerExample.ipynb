{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is my attempt at solving the clustering example in the \"Spark and Python for Big data with Pyspark\"\n",
    "# udemy course\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init('/home/ubuntu/spark-2.4.4-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType\n",
    "import pyspark.sql.functions as sparkf\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from functools import reduce\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from typing import List, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def create_session(name: str):\n",
    "    return SparkSession.builder.appName(name).getOrCreate()\n",
    "\n",
    "\n",
    "# TODO: put typehints in everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly define schema\n",
    "hacker_schema = [StructField(\"Session_Connection_Time\", IntegerType(), nullable=True), \n",
    "                StructField(\"Bytes Transferred\", DoubleType(), nullable=True),\n",
    "                StructField(\"Kali_Trace_Used\", IntegerType(), nullable=True),\n",
    "                StructField(\"Servers_Corrupted\", DoubleType(), nullable=True),\n",
    "                StructField(\"Pages_Corrupted\", IntegerType(), nullable=True),\n",
    "                StructField(\"Location\", StringType(), nullable=True),\n",
    "                StructField(\"WPM_Typing_Speed\", DoubleType(), nullable=True)]\n",
    "hacker_struct = StructType(fields=hacker_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe from csv\n",
    "hacker_session = create_session(\"hacker_example\")\n",
    "hacker_original_df = hacker_session.read.csv(\"hack_data.csv\", inferSchema=True, header=True)\n",
    "# create another dataframe that excludes the LOCATION column as it was suggested that wasn't useful\n",
    "hacker_no_location_df = hacker_original_df.drop(\"Location\")\n",
    "\n",
    "# hacker_original_df.head(1)\n",
    "# hacker_original_df.printSchema()\n",
    "# [Row(Session_Connection_Time=None, Bytes_Transferred=None, Kali_Trace_Used=None, Servers_Corrupted=None, Pages_Corrupted=None, Location=None, WPM_Typing_Speed=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical location data into numerical data, only need HANDLEINVALID = \"keep\" if column\n",
    "# has null values, still don't know why that works though or exactly what that does\n",
    "indexer = StringIndexer(inputCol=\"Location\", outputCol=\"Location_Index\")\n",
    "indexed_original = indexer.fit(hacker_original_df).transform(hacker_original_df)\n",
    "\n",
    "# and then remove the string Location column as cannot add string data into a features vector\n",
    "indexed_original = indexed_original.drop(\"Location\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity we will just have 2 transformed datasets. (1) with the location (2) without location. \n",
    "# I want keep both as I'd like to compare the clustering performance to confirm if location is relevant or not\n",
    "original_assembler = VectorAssembler(inputCols=indexed_original.columns, outputCol=\"features\")\n",
    "no_location_assembler = VectorAssembler(inputCols=hacker_no_location_df.columns, outputCol=\"features\")\n",
    "\n",
    "original_features = original_assembler.transform(indexed_original)\n",
    "no_location_features = no_location_assembler.transform(hacker_no_location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, WPM_Typing_Speed=72.37, Location_Index=54.0, features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37, 54.0]), scaled_features=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963, 1.059]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the features vector so that differing scales of each column doesn't impact clustering (especially\n",
    "# with the converted categorical column)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(original_features)\n",
    "final_original_data = scaler_model.transform(original_features)\n",
    "\n",
    "\n",
    "# same thing but with the df that has no location\n",
    "scaler_model = scaler.fit(no_location_features)\n",
    "final_no_location_data = scaler_model.transform(no_location_features)\n",
    "final_original_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterResults:\n",
    "    \"\"\"container class to put any relevant info into for clustering centers chosen\"\"\"\n",
    "    \n",
    "    _model_key = \"model_object\"\n",
    "    _df_key = \"original_dataframe\"\n",
    "    _features = \"feature_column_label\"\n",
    "    _silhouette_score = \"silhouette_score\"\n",
    "    _sse = \"sse_nearest_center\"\n",
    "    def __init__(self):\n",
    "        \"\"\"right now _RESULTS dict will be something like:\n",
    "            {<dataframe_label>: {\"model_object\": <model_object>, \"original_dataframe\": <df_object>, \n",
    "                                 \"feature_column_label\": <feature column label str>}}\n",
    "        \"\"\"\n",
    "        self._results = {}\n",
    "        \n",
    "    def add_result(self, model: KMeansModel, dataframe_label: str, original_dataframe: DataFrame, \n",
    "                   feature_column_label: str, silhouette_score: float, sse_nearest_center: float) -> None:\n",
    "        self._results[dataframe_label] = {ClusterResults._model_key: model, ClusterResults._df_key: original_dataframe,\n",
    "                                          ClusterResults._features: feature_column_label,\n",
    "                                          ClusterResults._silhouette_score: silhouette_score,\n",
    "                                          ClusterResults._sse: sse_nearest_center}\n",
    "        \n",
    "    def get_model(self, dataframe_label: str) -> Any:\n",
    "        return self._results[dataframe_label][ClusterResults._model_key]\n",
    "    \n",
    "    def get_num_centroids(self, dataframe_label: str) -> int:\n",
    "        model: KMeansModel = self.get_model(dataframe_label)\n",
    "        return len(model.clusterCenters())\n",
    "    \n",
    "    def get_centroids(self, dataframe_label: str) -> List[np.array]:\n",
    "        model: KMeansModel = self.get_model(dataframe_label)\n",
    "        return model.clusterCenters()\n",
    "    \n",
    "    def get_silhouette_score(self, dataframe_label: str) -> float:\n",
    "        return self._results[dataframe_label][ClusterResults._silhouette_score]\n",
    "    \n",
    "    def display_model_info(self, dataframe_label: str) -> None:\n",
    "        print(f\"For {dataframe_label} data the best performing cluster had {self.get_num_centroids(dataframe_label)} \\\n",
    "                centers and they are: {self.get_centroids(dataframe_label)}\")\n",
    "        print(f\"Silhouette score was: {self.get_silhouette_score(dataframe_label)}\")\n",
    "        \n",
    "    def display_all_models_info(self) -> None:\n",
    "        for dataframe_label in self._results:\n",
    "            self.display_model_info(dataframe_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# helper method that takes in list of tuples (<label for df>, <df>) and the labels for features column \n",
    "# that is the label for the column for the scaled features column\n",
    "def fit_and_evaluate(dataframes: List[Tuple[str, DataFrame]], features_col: str, \n",
    "                     max_centers_to_try: int=8, verbose: bool=False) -> ClusterResults:\n",
    "    \"\"\"fits cluster centers from the input data in DATAFRAMES, using the FEATURES_COL to as the features column\n",
    "    to cluster. Currently, makes decision on number of centers solely on silhouette score, but outputs both\n",
    "    silhouette and sum of squared distances to nearest center\"\"\"\n",
    "    cluster_results: ClusterResults = ClusterResults()\n",
    "    for df_label, df in dataframes:\n",
    "        models: List[KMeansModel] = []\n",
    "        # remember cannot make a 1 center Kmeans cluster model. As clustering meant to seperate into mulitiple groups    \n",
    "        # make multiple k means models and evaluate them, to keep things quick we'll just loop 1 - 5 centers\n",
    "        for i in range(2, max_centers_to_try, 1):\n",
    "            kmeans = KMeans(featuresCol=features_col).setK(i).setSeed(1)\n",
    "            model: KMeansModel = kmeans.fit(df)\n",
    "            predictions: Dataframe = model.transform(df)\n",
    "            models.append((model, predictions))\n",
    "\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        # evaluate the models (there's sum of squared distances metric which you can see its limitations \n",
    "        # as with more centers the distance gets smaller as we overfit with too many centers)\n",
    "        # The Silhouette analysis metric helps us shed light on the optimal number of centers\n",
    "        # Good example below of what the ClusteringEvaluator is doing and silhouette analysis\n",
    "        # https://runawayhorse001.github.io/LearningApacheSpark/clustering.html\n",
    "        # https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "        max_silhouette_score: float = -1\n",
    "        best_model: Kmeans = None\n",
    "        related_sse_nearest_center: float = 0\n",
    "        print(f\"evaluation metrics for dataset: {df_label}\")\n",
    "        for model, predictions in models:\n",
    "            print(f\"model with {len(model.clusterCenters())} center(s)\")\n",
    "            sse_nearest_center: float = model.computeCost(df)\n",
    "            print(f\"\\t sum of squared distances of points to their nearest center: {sse_nearest_center:,}\")\n",
    "            silhouette_score = evaluator.evaluate(predictions)\n",
    "            print(f\"\\t Silhouette analysis: {silhouette_score}\")\n",
    "            if silhouette_score > max_silhouette_score:\n",
    "                max_silhouette_score = silhouette_score\n",
    "                best_model = model\n",
    "                related_sse_nearest_center = sse_nearest_center\n",
    "        \n",
    "        cluster_results.add_result(best_model, df_label, df, features_col, max_silhouette_score,\n",
    "                                   related_sse_nearest_center)\n",
    "    \n",
    "    return cluster_results\n",
    "                \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation metrics for dataset: no_location\n",
      "model with 2 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 601.7707512676716\n",
      "\t Silhouette analysis: 0.6683623593283755\n",
      "model with 3 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 434.75507308487647\n",
      "\t Silhouette analysis: 0.30412315937808737\n",
      "model with 4 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 267.1336116887891\n",
      "\t Silhouette analysis: -0.04792891045570489\n",
      "model with 5 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 406.0381118469415\n",
      "\t Silhouette analysis: 0.10059152769091971\n",
      "model with 6 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 227.5888199292027\n",
      "\t Silhouette analysis: -0.10827640583392491\n",
      "model with 7 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 207.04933720005226\n",
      "\t Silhouette analysis: -0.13479542730344232\n",
      "evaluation metrics for dataset: with_location\n",
      "model with 2 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 934.2676092002957\n",
      "\t Silhouette analysis: 0.6559024043585371\n",
      "model with 3 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 766.2083651605144\n",
      "\t Silhouette analysis: 0.30145016868406393\n",
      "model with 4 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 681.323760145727\n",
      "\t Silhouette analysis: 0.3009008940616773\n",
      "model with 5 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 514.2167587717527\n",
      "\t Silhouette analysis: -0.04241721380209907\n",
      "model with 6 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 495.8428386437074\n",
      "\t Silhouette analysis: -0.10750198040872569\n",
      "model with 7 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 453.64343514097936\n",
      "\t Silhouette analysis: -0.09734706130122481\n",
      "For no_location data the best performing cluster had 2                 centers and they are: [array([1.26023837, 1.31829808, 0.99280765, 1.36491885, 2.5625043 ,\n",
      "       5.26676612]), array([2.99991988, 2.92319035, 1.05261534, 3.20390443, 4.51321315,\n",
      "       3.28474   ])]\n",
      "Silhouette score was: 0.6683623593283755\n",
      "For with_location data the best performing cluster had 2                 centers and they are: [array([2.99991988, 2.92319035, 1.05261534, 3.20390443, 4.51321315,\n",
      "       3.28474   , 1.23589466]), array([1.26023837, 1.31829808, 0.99280765, 1.36491885, 2.5625043 ,\n",
      "       5.26676612, 1.31351975])]\n",
      "Silhouette score was: 0.6559024043585371\n"
     ]
    }
   ],
   "source": [
    "# put the dataframes into a list to iterate through to keep code DRY. Still don't have a great way to \n",
    "# retrieve alias of DF\n",
    "dataframes_to_analyze = [(\"no_location\", final_no_location_data), (\"with_location\", final_original_data)]\n",
    "\n",
    "# fit the dataframes from above and get the corresponding results\n",
    "results: ClusterResults = fit_and_evaluate(dataframes_to_analyze, \"scaled_features\", verbose=True)\n",
    "    \n",
    "# print the number of centroids and where they are (no necessarily interesting) for each\n",
    "results.display_all_models_info()\n",
    "\n",
    "# ANSWER to the Udemy project question:\n",
    "# Based on the output you should see after running this block. Seems that both dataframes with and without\n",
    "# location both have the best silhouette scores when there are only 2 centers. Thus, this implies there\n",
    "# were only TWO hackers and the third hacker was not involved\n",
    "\n",
    "# NOTE: silhouette score between dataframes with and without location are basically negligible which confirms\n",
    "# the assumption that Joser Padilla (Udemy) says we can make where location is unrelated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
