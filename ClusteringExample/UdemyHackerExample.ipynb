{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is my attempt at solving the clustering example in the \"Spark and Python for Big data with Pyspark\"\n",
    "# udemy course\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init('/home/ubuntu/spark-2.4.4-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType\n",
    "import pyspark.sql.functions as sparkf\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from functools import reduce\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "def create_session(name: str):\n",
    "    return SparkSession.builder.appName(name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly define schema\n",
    "hacker_schema = [StructField(\"Session_Connection_Time\", IntegerType(), nullable=True), \n",
    "                StructField(\"Bytes Transferred\", DoubleType(), nullable=True),\n",
    "                StructField(\"Kali_Trace_Used\", IntegerType(), nullable=True),\n",
    "                StructField(\"Servers_Corrupted\", DoubleType(), nullable=True),\n",
    "                StructField(\"Pages_Corrupted\", IntegerType(), nullable=True),\n",
    "                StructField(\"Location\", StringType(), nullable=True),\n",
    "                StructField(\"WPM_Typing_Speed\", DoubleType(), nullable=True)]\n",
    "hacker_struct = StructType(fields=hacker_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe from csv\n",
    "hacker_session = create_session(\"hacker_example\")\n",
    "hacker_original_df = hacker_session.read.csv(\"hack_data.csv\", inferSchema=True, header=True)\n",
    "# create another dataframe that excludes the LOCATION column as it was suggested that wasn't useful\n",
    "hacker_no_location_df = hacker_original_df.drop(\"Location\")\n",
    "\n",
    "# hacker_original_df.head(1)\n",
    "# hacker_original_df.printSchema()\n",
    "# [Row(Session_Connection_Time=None, Bytes_Transferred=None, Kali_Trace_Used=None, Servers_Corrupted=None, Pages_Corrupted=None, Location=None, WPM_Typing_Speed=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical location data into numerical data, only need HANDLEINVALID = \"keep\" if column\n",
    "# has null values, still don't know why that works though or exactly what that does\n",
    "indexer = StringIndexer(inputCol=\"Location\", outputCol=\"Location_Index\")\n",
    "indexed_original = indexer.fit(hacker_original_df).transform(hacker_original_df)\n",
    "\n",
    "# and then remove the string Location column as cannot add string data into a features vector\n",
    "indexed_original = indexed_original.drop(\"Location\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity we will just have 2 transformed datasets. (1) with the location (2) without location. \n",
    "# I want keep both as I'd like to compare the clustering performance to confirm if location is relevant or not\n",
    "original_assembler = VectorAssembler(inputCols=indexed_original.columns, outputCol=\"features\")\n",
    "no_location_assembler = VectorAssembler(inputCols=hacker_no_location_df.columns, outputCol=\"features\")\n",
    "\n",
    "original_features = original_assembler.transform(indexed_original)\n",
    "no_location_features = no_location_assembler.transform(hacker_no_location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, WPM_Typing_Speed=72.37, Location_Index=54.0, features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37, 54.0]), scaled_features=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963, 1.059]))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the features vector so that differing scales of each column doesn't impact clustering (especially\n",
    "# with the converted categorical column)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(original_features)\n",
    "final_original_data = scaler_model.transform(original_features)\n",
    "\n",
    "\n",
    "# same thing but with the df that has no location\n",
    "scaler_model = scaler.fit(no_location_features)\n",
    "final_no_location_data = scaler_model.transform(no_location_features)\n",
    "final_original_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method that takes in list of tuples (<label for df>, <df>) and the labels for features column \n",
    "# that is the label for the column for the scaled features column\n",
    "def fit_and_evaluate(dataframes, features_col):\n",
    "    for df_label, df in dataframes:\n",
    "        models_original = []\n",
    "        # remember cannot make a 1 center Kmeans cluster model. As clustering meant to seperate into mulitiple groups    \n",
    "        # make multiple k means models and evaluate them, to keep things quick we'll just loop 1 - 5 centers\n",
    "        for i in range(2, 8, 1):\n",
    "            kmeans = KMeans(featuresCol=features_col).setK(i).setSeed(1)\n",
    "            model = kmeans.fit(df)\n",
    "            predictions = model.transform(df)\n",
    "            models_original.append((model, predictions))\n",
    "\n",
    "        evaluator = ClusteringEvaluator()\n",
    "        # evaluate the models (there's sum of squared distances metric which you can see its limitations \n",
    "        # as with more centers the distance gets smaller as we overfit with too many centers)\n",
    "        # The Silhouette analysis metric helps us shed light on the optimal number of centers\n",
    "        # Good example below of what the ClusteringEvaluator is doing and silhouette analysis\n",
    "        # https://runawayhorse001.github.io/LearningApacheSpark/clustering.html\n",
    "        # https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "        print(f\"evaluation metrics for dataset: {df_label}\")\n",
    "        for model, predictions in models_original:\n",
    "            print(f\"model with {len(model.clusterCenters())} center(s)\")\n",
    "            print(f\"\\t sum of squared distances of points to their nearest center: {model.computeCost(df):,}\")\n",
    "            print(f\"\\t Silhouette analysis: {evaluator.evaluate(predictions)}\")\n",
    "    \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation metrics for dataset: no_location\n",
      "model with 2 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 601.7707512676716\n",
      "\t Silhouette analysis: 0.6683623593283755\n",
      "model with 3 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 434.75507308487647\n",
      "\t Silhouette analysis: 0.30412315937808737\n",
      "model with 4 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 267.1336116887891\n",
      "\t Silhouette analysis: -0.04792891045570489\n",
      "model with 5 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 406.0381118469415\n",
      "\t Silhouette analysis: 0.10059152769091971\n",
      "model with 6 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 227.5888199292027\n",
      "\t Silhouette analysis: -0.10827640583392491\n",
      "model with 7 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 207.04933720005226\n",
      "\t Silhouette analysis: -0.13479542730344232\n",
      "evaluation metrics for dataset: with_location\n",
      "model with 2 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 934.2676092002957\n",
      "\t Silhouette analysis: 0.6559024043585371\n",
      "model with 3 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 766.2083651605144\n",
      "\t Silhouette analysis: 0.30145016868406393\n",
      "model with 4 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 681.323760145727\n",
      "\t Silhouette analysis: 0.3009008940616773\n",
      "model with 5 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 514.2167587717527\n",
      "\t Silhouette analysis: -0.04241721380209907\n",
      "model with 6 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 495.8428386437074\n",
      "\t Silhouette analysis: -0.10750198040872569\n",
      "model with 7 center(s)\n",
      "\t sum of squared distances of points to their nearest center: 453.64343514097936\n",
      "\t Silhouette analysis: -0.09734706130122481\n"
     ]
    }
   ],
   "source": [
    "# put the dataframes into a list to iterate through to keep code DRY. Still don't have a great way to \n",
    "# retrieve alias of DF\n",
    "dataframes_to_analyze = [(\"no_location\", final_no_location_data), (\"with_location\", final_original_data)]\n",
    "\n",
    "fit_and_evaluate(dataframes_to_analyze, \"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
