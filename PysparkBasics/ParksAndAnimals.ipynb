{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/home/ubuntu/spark-2.4.4-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, StructField, StructType\n",
    "import pyspark.sql.functions as sparkf\n",
    "from functools import reduce\n",
    "\n",
    "# below is a notebook that goes through some data provided from: https://www.kaggle.com/jonathanbouchet/biodiversity-in-us-national-parks-2016\n",
    "# and it's just a way for me to discover how to use some pyspark basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables I use over and over again\n",
    "PARK_NAME_KEY = \"Park Name\"\n",
    "def create_session(name: str):\n",
    "    return SparkSession.builder.appName(name).getOrCreate()\n",
    "\n",
    "def get_null_rows(df):\n",
    "    # returns a df that includes all the rows with any null values     \n",
    "    return df.where(reduce(lambda x, y: x | y, [sparkf.col(x).isNull() for x in df.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session(\"parks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicityly create schemas to ensure the data is what we expect it to be, don't want to rely on spark inference\n",
    "parks_schema = [StructField(\"Park Code\", StringType(), True), StructField(\"Park Name\", StringType(), True), \n",
    "                StructField(\"State\", StringType(), True), StructField(\"Acres\", IntegerType(), True),\n",
    "               StructField(\"Latitude\", DoubleType(), True), StructField(\"Longitude\", DoubleType(), True)]\n",
    "park_struct = StructType(fields=parks_schema)\n",
    "\n",
    "species_schema = [StructField(\"Species ID\", StringType(), True), StructField(\"Park Name\", StringType(), True),\n",
    "                  StructField(\"Category\", StringType(), True), StructField(\"Order\", StringType(), True),\n",
    "                  StructField(\"Family\", StringType(), True), StructField(\"Scientific Name\", StringType(), True),\n",
    "                  StructField(\"Common Names\", StringType(), True), StructField(\"Record Status\", StringType(), True),\n",
    "                  StructField(\"Occurence\", StringType(), True), StructField(\"Nativeness\", StringType(), True),\n",
    "                  StructField(\"Abundance\", StringType(), True), StructField(\"Seasonality\", StringType(), True),\n",
    "                  StructField(\"Conservation Status\", StringType(), True)]\n",
    "species_struct = StructType(fields=species_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from csv files\n",
    "parks_df = spark.read.csv(\"parks.csv\", schema=park_struct, header=True)\n",
    "species_df = spark.read.csv(\"species.csv\", schema=species_struct, header=True)\n",
    "dfs_to_analyze = [(\"parks\", parks_df), (\"species\", species_df)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to rectify null values in species dataframe. There are 116551 rows with null values\n"
     ]
    }
   ],
   "source": [
    "# check if any of the data frames have null values\n",
    "for df_name, df in dfs_to_analyze:\n",
    "    any_nulls = get_null_rows(df)\n",
    "    if any_nulls.count() != 0:\n",
    "        print(f\"Need to rectify null values in {df_name} dataframe. There are {any_nulls.count()} rows with null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prelim data work in order to get a break down of state acreage and number of parks in each state\n",
    "class StateInfo:\n",
    "    \n",
    "    def __init__(self, state_acreages, parks_per_state):\n",
    "        self.state_acreages = state_acreages\n",
    "        self.parks_per_state = parks_per_state\n",
    "\n",
    "def state_summary(df):\n",
    "    state_acreages = {}\n",
    "    parks_per_state = {}\n",
    "\n",
    "    for row in df.rdd.collect():\n",
    "        # thought could have attributes be case insensitive - unsure\n",
    "        states = row.State\n",
    "        # since I know if multiple states then they are delimited with a \", \" (comma and then space)\n",
    "        states = states.split(\", \")\n",
    "        acreage = row.Acres / len(states)\n",
    "        for state in states:\n",
    "            if state in state_acreages:\n",
    "                state_acreages[state] += acreage\n",
    "                parks_per_state[state] += 1\n",
    "            else:\n",
    "                state_acreages[state] = acreage\n",
    "                parks_per_state[state] = 1\n",
    "    \n",
    "    return StateInfo(state_acreages, parks_per_state)\n",
    "\n",
    "def format_num_values_dict(data_dict):\n",
    "    str_output_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        rounded_num = int(round(data_dict[key], 0))\n",
    "        str_output_dict[key] = f\"{rounded_num:,}\"\n",
    "        \n",
    "    return str_output_dict\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Acreage by state (approx, parks spanning across multiple states \n",
      "      evenly distributed across those states:\n",
      " {'ME': '47,390', 'UT': '838,454', 'SD': '271,051', 'TX': '887,579', 'FL': '1,746,163', 'CO': '393,884', 'NM': '46,766', 'CA': '5,282,470', 'SC': '26,546', 'OR': '183,224', 'OH': '32,950', 'AK': '31,159,251', 'NV': '2,447,636', 'MT': '1,753,502', 'AZ': '1,402,376', 'TN': '260,745', 'NC': '260,745', 'WY': '1,049,925', 'HI': '352,525', 'AR': '5,550', 'MI': '571,790', 'KY': '52,830', 'WA': '1,663,057', 'VA': '199,045', 'ND': '70,447', 'MN': '218,200', 'ID': '739,930'}\n",
      "Parks per state: {'ME': 1, 'UT': 5, 'SD': 2, 'TX': 2, 'FL': 3, 'CO': 4, 'NM': 1, 'CA': 8, 'SC': 1, 'OR': 1, 'OH': 1, 'AK': 8, 'NV': 2, 'MT': 2, 'AZ': 3, 'TN': 1, 'NC': 1, 'WY': 2, 'HI': 2, 'AR': 1, 'MI': 1, 'KY': 1, 'WA': 3, 'VA': 1, 'ND': 1, 'MN': 1, 'ID': 1}\n"
     ]
    }
   ],
   "source": [
    "# by state park summary stats\n",
    "state_info = state_summary(parks_df)\n",
    "acreage_output = format_num_values_dict(state_info.state_acreages)\n",
    "print(f\"Total Acreage by state (approx, parks spanning across multiple states \\n\\\n",
    "      evenly distributed across those states:\\n {acreage_output}\")\n",
    "print(f\"Parks per state: {state_info.parks_per_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total national park acreage in US: 51,964,032\n",
      "Number of US National Parks: 56\n"
     ]
    }
   ],
   "source": [
    "# some simple summary data points\n",
    "agg_row = parks_df.agg(sparkf.sum(\"Acres\").alias(\"Total Acreage\"))\n",
    "total_acreage = agg_row.head()[0]\n",
    "print(f\"Total national park acreage in US: {total_acreage:,}\")\n",
    "print(f\"Number of US National Parks: {parks_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Acadia National Park', 'Arches National Park', 'Badlands National Park', 'Big Bend National Park', 'Biscayne National Park', 'Black Canyon of the Gunnison National Park', 'Bryce Canyon National Park', 'Canyonlands National Park', 'Capitol Reef National Park', 'Carlsbad Caverns National Park', 'Channel Islands National Park', 'Congaree National Park', 'Crater Lake National Park', 'Cuyahoga Valley National Park', 'Death Valley National Park', 'Denali National Park and Preserve', 'Dry Tortugas National Park', 'Everglades National Park', 'Gates Of The Arctic National Park and Preserve', 'Glacier Bay National Park and Preserve', 'Glacier National Park', 'Grand Canyon National Park', 'Grand Teton National Park', 'Great Basin National Park', 'Great Sand Dunes National Park and Preserve', 'Great Smoky Mountains National Park', 'Guadalupe Mountains National Park', 'Haleakala National Park', 'Hawaii Volcanoes National Park', 'Hot Springs National Park', 'Isle Royale National Park', 'Joshua Tree National Park', 'Katmai National Park and Preserve', 'Kenai Fjords National Park', 'Kobuk Valley National Park', 'Lake Clark National Park and Preserve', 'Lassen Volcanic National Park', 'Mammoth Cave National Park', 'Mesa Verde National Park', 'Mount Rainier National Park', 'North Cascades National Park', 'Olympic National Park', 'Petrified Forest National Park', 'Pinnacles National Park', 'Redwood National Park', 'Rocky Mountain National Park', 'Saguaro National Park', 'Sequoia and Kings Canyon National Parks', 'Shenandoah National Park', 'Theodore Roosevelt National Park', 'Voyageurs National Park', 'Wind Cave National Park', 'Wrangell - St Elias National Park and Preserve', 'Yellowstone National Park', 'Yosemite National Park', 'Zion National Park']\n"
     ]
    }
   ],
   "source": [
    "# sorted list of park names\n",
    "print(sorted([row[0] for row in parks_df.distinct().select(PARK_NAME_KEY).collect()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parks have species info\n"
     ]
    }
   ],
   "source": [
    "# joining the two dataframes and only selecting the columns we want \n",
    "joined_dataframe = parks_df.join(species_df, on=[PARK_NAME_KEY] , how=\"inner\").select([PARK_NAME_KEY, \"Acres\", \"Category\", \"Order\", \"Family\"])\n",
    "park_group_species_count = joined_dataframe.groupBy(PARK_NAME_KEY).count()\n",
    "\n",
    "# checking if some parks have no species data\n",
    "if park_group_species_count.count() != parks_df.count():\n",
    "    parks_species_info = [row[PARK_NAME_KEY] for row in park_group_species_count.select(PARK_NAME_KEY).distinct().collect()]\n",
    "    national_parks_no_info = [row[PARK_NAME_KEY] for row in parks_df.select(PARK_NAME_KEY).distinct().collect() if row[PARK_NAME_KEY] not in parks_species_info]\n",
    "    print(f\"{len(national_parks_no_info)} number of parks have no species info: {national_parks_no_info}\")\n",
    "else:\n",
    "    print(\"All parks have species info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zion National Park has 1796 different species\n",
      "Glacier National Park has 2556 different species\n",
      "Lassen Volcanic National Park has 1797 different species\n"
     ]
    }
   ],
   "source": [
    "# Depending on what park names you input into PARK_VARIABLE it outputs the number of different species within\n",
    "# the park\n",
    "PARK_VARIABLE = [\"Lassen Volcanic National Park\", 'Zion National Park', 'Glacier National Park'] # change depending what park(s) you want to see\n",
    "grouped_df = joined_dataframe.groupBy(PARK_NAME_KEY).count()\n",
    "number_of_species = grouped_df.filter(grouped_df[PARK_NAME_KEY].isin(PARK_VARIABLE)).collect()\n",
    "for result in number_of_species:\n",
    "    print(f\"{result[PARK_NAME_KEY]} has {result['count']} different species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zion National Park category breakdown:\n",
      "\t Algae: None\n",
      "\t Amphibian: 7\n",
      "\t Bird: 301\n",
      "\t Crab/Lobster/Shrimp: None\n",
      "\t Fish: 15\n",
      "\t Fungi: None\n",
      "\t Insect: None\n",
      "\t Invertebrate: None\n",
      "\t Mammal: 80\n",
      "\t Nonvascular Plant: None\n",
      "\t Reptile: 30\n",
      "\t Slug/Snail: None\n",
      "\t Spider/Scorpion: None\n",
      "\t Vascular Plant: 1363\n",
      "Glacier National Park category breakdown:\n",
      "\t Algae: 2\n",
      "\t Amphibian: 6\n",
      "\t Bird: 277\n",
      "\t Crab/Lobster/Shrimp: 6\n",
      "\t Fish: 27\n",
      "\t Fungi: 276\n",
      "\t Insect: 197\n",
      "\t Invertebrate: 2\n",
      "\t Mammal: 69\n",
      "\t Nonvascular Plant: 404\n",
      "\t Reptile: 4\n",
      "\t Slug/Snail: 20\n",
      "\t Spider/Scorpion: None\n",
      "\t Vascular Plant: 1266\n",
      "Lassen Volcanic National Park category breakdown:\n",
      "\t Algae: 2\n",
      "\t Amphibian: 17\n",
      "\t Bird: 245\n",
      "\t Crab/Lobster/Shrimp: 15\n",
      "\t Fish: 20\n",
      "\t Fungi: 51\n",
      "\t Insect: 95\n",
      "\t Invertebrate: 18\n",
      "\t Mammal: 100\n",
      "\t Nonvascular Plant: 160\n",
      "\t Reptile: 22\n",
      "\t Slug/Snail: 6\n",
      "\t Spider/Scorpion: None\n",
      "\t Vascular Plant: 1046\n"
     ]
    }
   ],
   "source": [
    "# takes PARK_VARIABLE and does some deeper analysis on species within each park\n",
    "PARK_VARIABLE = [\"Lassen Volcanic National Park\", 'Zion National Park', 'Glacier National Park'] # change depending what park(s) you want to see\n",
    "category_pivot_df = joined_dataframe.groupBy(PARK_NAME_KEY).pivot(\"Category\").count()\n",
    "filtered_category_pivot_df = category_pivot_df.filter(category_pivot_df[PARK_NAME_KEY].isin(PARK_VARIABLE))\n",
    "# filtered_category_pivot_df.show() #commented out as too wide so will print instead\n",
    "for row in filtered_category_pivot_df.collect():\n",
    "    print(f\"{row[PARK_NAME_KEY]} category breakdown:\") # note need keys to be in single quotes - think b/c my f strings use dbl quotes\n",
    "    for key, value in row.asDict().items():\n",
    "          if key != PARK_NAME_KEY:\n",
    "                print(f\"\\t {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
